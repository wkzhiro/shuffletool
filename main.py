import streamlit as st
import pandas as pd
import numpy as np
from openai import AzureOpenAI
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import os
from dotenv import load_dotenv
from collections import Counter

warnings.filterwarnings('ignore')

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv('.env.local')

def can_convert_to_float(x):
    """xãŒæµ®å‹•å°æ•°ç‚¹æ•°ã«å¤‰æ›å¯èƒ½ãªã‚‰True, ãã‚Œä»¥å¤–False"""
    try:
        float(x)
        return True
    except:
        return False

def group_selection(unique_keys, label):
    """
    1ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«ã¤ã„ã¦ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã™ã‚‹å‡¦ç†ã€‚
    ã“ã“ã§ã¯æ•°å€¤ã‹ã©ã†ã‹åˆ¤å®šã—ã¤ã¤ã€ã™ã¹ã¦æ–‡å­—åˆ—ã¨ã—ã¦ã‚½ãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
    """
    # ã¾ãšNaNã¯é™¤å¤–
    unique_keys = [x for x in unique_keys if pd.notna(x)]

    # ã™ã¹ã¦æ–‡å­—åˆ—ã«å¤‰æ›ã—ã€å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤ï¼ˆstripï¼‰
    unique_keys_str = [str(x).strip() for x in unique_keys]

    # æ•°å€¤åˆ¤å®š â†’ ä»Šå›ã¯ä¾‹ã¨ã—ã¦ã€Œæ•°å€¤ã‚½ãƒ¼ãƒˆ or æ–‡å­—åˆ—ã‚½ãƒ¼ãƒˆã€æ®‹ã—ã¾ã™ãŒ
    # å…¨éƒ¨æ–‡å­—åˆ—ã‚½ãƒ¼ãƒˆã«ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ãã ã•ã„
    if all(can_convert_to_float(x) for x in unique_keys_str):
        sorted_keys = sorted(unique_keys_str, key=lambda x: float(x))
    else:
        sorted_keys = sorted(unique_keys_str)

    groups = []
    remaining_keys = sorted_keys.copy()

    st.write(f"### {label} ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘")
    # st.write("#### ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ (ã‚½ãƒ¼ãƒˆå¾Œ) =", remaining_keys)

    for i in range(5):
        group = st.multiselect(
            f"Group {i + 1} ({label})",
            options=remaining_keys,
            default=[]
        )
        groups.append(group)
        # é¸æŠã•ã‚ŒãŸã‚­ãƒ¼ã‚’ remaining_keys ã‹ã‚‰é™¤å¤–
        remaining_keys = [k for k in remaining_keys if k not in group]
        if not remaining_keys:
            break

    # # ãƒ‡ãƒãƒƒã‚°: ã©ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã©ã‚“ãªã‚­ãƒ¼ãŒå…¥ã£ãŸã‹
    # st.write(f"=== DEBUG: {label} groups ===", groups)
    return groups

def assign_groups(df_col, groups):
    """
    ä¾‹: groups=[["A","B"],["C"],["D","E"]]
     => "A","B" ã¯ catX_group="Group 1"
        "C" ã¯ catX_group="Group 2"
        "D","E" ã¯ catX_group="Group 3"
    """
    # df_col ã‚‚å…¨ã¦æ–‡å­—åˆ—ï¼‹stripã—ã¦ãŠã
    df_col_str = df_col.astype(str).str.strip()

    group_map = {}
    for idx, group_list in enumerate(groups):
        for key in group_list:
            # key è‡ªä½“ã‚‚ strip() ã—ã¦ãŠãã¨å®‰å…¨
            k = key.strip()
            group_map[k] = f"Group {idx + 1}"

    # st.write("=== DEBUG: group_map ===")
    # st.write(group_map)

    # ãƒãƒƒãƒ”ãƒ³ã‚°
    mapped_series = df_col_str.map(group_map)

    # # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸçµæœãŒã©ã†ãªã£ãŸã‹è¡¨ç¤º
    # st.write("=== DEBUG: assign_groups result (sample) ===")
    # st.write(pd.DataFrame({
    #     "original": df_col.head(20),      # å…ƒã®Excelåˆ— (ãã®ã¾ã¾)
    #     "original_stripped": df_col_str.head(20),  # stripå¾Œ
    #     "mapped": mapped_series.head(20)
    # }))

    return mapped_series

def get_embeddings(texts, client, deployment_name):
    """
    Azure OpenAIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’å–å¾—
    """
    if client is None:
        st.error("Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None
        
    try:
        embeddings = []
        for text in texts:
            response = client.embeddings.create(
                input=str(text),
                model=deployment_name
            )
            embeddings.append(response.data[0].embedding)
        return np.array(embeddings)
    except Exception as e:
        st.error(f"Embeddingå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def analyze_column_type(series):
    """
    åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤å®šï¼ˆãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ vs ã‚¿ã‚°ç³»ï¼‰
    """
    # NaNã‚’é™¤å¤–
    valid_data = series.dropna()
    if len(valid_data) == 0:
        return "tag", "ç©ºã®ãƒ‡ãƒ¼ã‚¿"
    
    # æ•°å€¤åˆ—ã®å ´åˆã¯æ˜ã‚‰ã‹ã«ã‚¿ã‚°ç³»
    if pd.api.types.is_numeric_dtype(series):
        return "tag", "æ•°å€¤ãƒ‡ãƒ¼ã‚¿"
    
    # æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
    texts = valid_data.astype(str)
    
    # 1. æ–‡å­—åˆ—é•·åˆ†æ
    avg_length = texts.str.len().mean()
    
    # 2. ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã®å‰²åˆ
    unique_ratio = len(texts.unique()) / len(texts)
    
    # 3. é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®å‰²åˆï¼ˆ30æ–‡å­—ä»¥ä¸Šï¼‰
    long_text_ratio = (texts.str.len() >= 30).mean()
    
    # 4. å¥èª­ç‚¹ã‚’å«ã‚€å‰²åˆ
    punctuation_ratio = texts.str.contains(r'[ã€‚ã€ï¼ï¼Ÿ.,!?]', regex=True).mean()
    
    # 5. ç©ºç™½ã‚’å«ã‚€å‰²åˆï¼ˆè¤‡æ•°å˜èªï¼‰
    space_ratio = texts.str.contains(r'\s+', regex=True).mean()
    
    # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
    free_text_score = 0
    reasons = []
    
    if avg_length >= 20:
        free_text_score += 2
        reasons.append(f"å¹³å‡æ–‡å­—æ•°: {avg_length:.1f}")
    
    if unique_ratio >= 0.7:
        free_text_score += 2
        reasons.append(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤å‰²åˆ: {unique_ratio:.1%}")
    
    if long_text_ratio >= 0.3:
        free_text_score += 2
        reasons.append(f"é•·æ–‡å‰²åˆ: {long_text_ratio:.1%}")
    
    if punctuation_ratio >= 0.3:
        free_text_score += 1
        reasons.append(f"å¥èª­ç‚¹å«æœ‰ç‡: {punctuation_ratio:.1%}")
    
    if space_ratio >= 0.5:
        free_text_score += 1
        reasons.append(f"ç©ºç™½å«æœ‰ç‡: {space_ratio:.1%}")
    
    # åˆ¤å®šçµæœ
    if free_text_score >= 4:
        return "free_text", f"ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®š (ã‚¹ã‚³ã‚¢:{free_text_score}) - " + ", ".join(reasons)
    else:
        return "tag", f"ã‚¿ã‚°ç³»åˆ¤å®š (ã‚¹ã‚³ã‚¢:{free_text_score}) - " + ", ".join(reasons)

def auto_detect_text_columns(df):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã®ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’è‡ªå‹•æ¤œå‡º
    """
    text_columns = []
    analysis_results = {}
    
    for i, col in enumerate(df.columns):
        if i == 0:  # æœ€åˆã®åˆ—ï¼ˆé€šå¸¸ã¯IDï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
            
        col_type, reason = analyze_column_type(df[col])
        analysis_results[f"åˆ—{i+1}: {col}"] = {
            "type": col_type,
            "reason": reason,
            "index": i
        }
        
        if col_type == "free_text":
            text_columns.append(i)
    
    return text_columns, analysis_results

def calculate_text_similarity_groups(text_series, client, deployment_name):
    """
    Azure OpenAIã®embeddingã‚’ä½¿ç”¨ã—ã¦ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€ä»®æƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
    """
    # NaNã‚’ç©ºæ–‡å­—åˆ—ã«ç½®æ›
    texts = text_series.fillna("").astype(str)
    
    # ç©ºæ–‡å­—åˆ—ã‚„çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    valid_texts = [t for t in texts if len(t.strip()) > 0]
    if len(valid_texts) < 2:
        return pd.Series(["TextGroup 1"] * len(texts), index=text_series.index)
    
    # Azure OpenAIã§embeddingã‚’å–å¾—
    st.info("ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ä¸­...")
    embeddings = get_embeddings(texts, client, deployment_name)
    
    if embeddings is None:
        st.warning("embeddingè¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å…¨å“¡ã‚’åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—ã«ã—ã¾ã™ã€‚")
        return pd.Series(["TextGroup 1"] * len(texts), index=text_series.index)
    
    try:
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—
        similarity_matrix = cosine_similarity(embeddings)
        
        # ç°¡æ˜“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆé–¾å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
        similarity_threshold = 0.7  # Azure OpenAIã®embeddingã¯é«˜å“è³ªãªã®ã§é–¾å€¤ã‚’é«˜ã‚ã«è¨­å®š
        text_groups = pd.Series(["TextGroup Other"] * len(texts), index=text_series.index)
        group_counter = 1
        assigned = set()
        
        for i in range(len(texts)):
            if i in assigned:
                continue
                
            # ç¾åœ¨ã®ãƒ†ã‚­ã‚¹ãƒˆã¨é¡ä¼¼ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
            similar_indices = [i]
            for j in range(i + 1, len(texts)):
                if j not in assigned and similarity_matrix[i][j] > similarity_threshold:
                    similar_indices.append(j)
            
            # ã‚°ãƒ«ãƒ¼ãƒ—ã«å‰²ã‚Šå½“ã¦
            if len(similar_indices) >= 2:  # 2äººä»¥ä¸Šã§ã‚°ãƒ«ãƒ¼ãƒ—å½¢æˆ
                group_name = f"TextGroup {group_counter}"
                for idx in similar_indices:
                    text_groups.iloc[idx] = group_name
                    assigned.add(idx)
                group_counter += 1
            else:
                assigned.add(i)
        
        return text_groups
        
    except Exception as e:
        st.warning(f"ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.Series(["TextGroup 1"] * len(texts), index=text_series.index)

def create_advanced_similarity_matrix(df, text_col1, text_col2=None, client=None, deployment_name=None):
    """
    ã€ä¿®æ­£ç‰ˆã€‘é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ç”¨ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’ä½œæˆ
    â˜… Embeddingã®çµæœã‚’è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ 
    """
    st.info("é¡ä¼¼åº¦è¡Œåˆ—ã‚’ä½œæˆä¸­...")
    
    # ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
    texts = []
    for idx in df.index:
        if text_col2 is None:
            # å˜ä¸€åˆ—ã§ã®é¡ä¼¼åº¦
            text = str(df.iloc[idx, text_col1]) if pd.notna(df.iloc[idx, text_col1]) else ""
            texts.append(text)
        else:
            # 2åˆ—çµåˆã§ã®é¡ä¼¼åº¦
            text1 = str(df.iloc[idx, text_col1]) if pd.notna(df.iloc[idx, text_col1]) else ""
            text2 = str(df.iloc[idx, text_col2]) if pd.notna(df.iloc[idx, text_col2]) else ""
            combined_text = f"{text1} [SEP] {text2}"
            texts.append(combined_text)
    
    # Azure OpenAIã‚’è©¦ã™
    if client and deployment_name:
        try:
            st.write("Azure OpenAI APIã‚’å‘¼ã³å‡ºã—ã¦Embeddingã‚’å–å¾—ã—ã¾ã™...")
            embeddings = get_embeddings_batch(texts, client, deployment_name)
            
            if embeddings is not None:
                # --- ã“ã“ã‹ã‚‰ãŒè¿½åŠ éƒ¨åˆ† ---
                st.success(f"âœ… Embeddingã®å–å¾—ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
                st.info(f"å–å¾—ã—ãŸãƒ™ã‚¯ãƒˆãƒ«æ•°: {embeddings.shape[0]}, å„ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°: {embeddings.shape[1]}")

                # é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—
                similarity_matrix = cosine_similarity(embeddings)

                # è©³ç´°ã‚’è¦‹ãŸã„äººå‘ã‘ã«ã€ãƒ™ã‚¯ãƒˆãƒ«ã¨é¡ä¼¼åº¦è¡Œåˆ—ã‚’å±•é–‹è¡¨ç¤º
                with st.expander("ã‚¯ãƒªãƒƒã‚¯ã—ã¦Embeddingã¨é¡ä¼¼åº¦è¡Œåˆ—ã®è©³ç´°ã‚’ç¢ºèª"):
                    st.write("â–¼ å–å¾—ã—ãŸEmbeddingãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
                    st.dataframe(pd.DataFrame(embeddings).head())
                    st.write("â–¼ è¨ˆç®—ã•ã‚ŒãŸé¡ä¼¼åº¦è¡Œåˆ—ï¼ˆæœ€åˆã®5x5éƒ¨åˆ†ï¼‰:")
                    st.dataframe(pd.DataFrame(similarity_matrix).head())
                # --- è¿½åŠ éƒ¨åˆ†ã“ã“ã¾ã§ ---
                
                mode = "å˜ä¸€åˆ—" if text_col2 is None else "2åˆ—çµåˆ"
                st.success(f"Azure OpenAIã«ã‚ˆã‚‹é¡ä¼¼åº¦è¡Œåˆ—ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚ï¼ˆ{mode}ï¼‰")
                return similarity_matrix
        except Exception as e:
            st.warning(f"Azure OpenAIå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}. TF-IDFã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
    
    # TF-IDFãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        st.write("TF-IDFã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
        
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(texts)
        similarity_matrix = cosine_similarity(tfidf_matrix)
        mode = "å˜ä¸€åˆ—" if text_col2 is None else "2åˆ—çµåˆ"
        st.success(f"TF-IDFã«ã‚ˆã‚‹é¡ä¼¼åº¦è¡Œåˆ—ã‚’ä½œæˆã—ã¾ã—ãŸï¼ˆ{mode}ï¼‰")
        return similarity_matrix
    except Exception as e:
        st.error(f"é¡ä¼¼åº¦è¡Œåˆ—ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def get_embeddings_batch(texts, client, deployment_name, batch_size=10):
    """ãƒãƒƒãƒã§embeddingã‚’å–å¾—"""
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        try:
            response = client.embeddings.create(
                model=deployment_name,
                input=batch_texts
            )
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)
        except Exception as e:
            st.error(f"ãƒãƒƒãƒ {i//batch_size + 1} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    return np.array(all_embeddings)

def calculate_advanced_group_score(df, group_indices, similarity_matrix, priority_options, score_map, category_strategies, w_label=1.0, w_sim=1.0):
    """
    ã€ä¿®æ­£ç‰ˆã€‘é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
    ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°ã§éƒ¨åˆ†ç‚¹ã‚’è€ƒæ…®ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ 
    """
    if len(group_indices) < 2:
        return 0

    # 1. ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢è¨ˆç®—
    category_score = 0
    for priority in priority_options:
        if priority not in score_map:
            continue
            
        base_points = score_map[priority]
        
        cat_num = int(priority.replace('cat', '').replace('_group', ''))
        strategy = category_strategies.get(cat_num, 'diversity')
        
        col_index = df.columns.get_loc(priority)
        values = [df.iloc[idx, col_index] for idx in group_indices]
        
        # NaNãªã©ã‚’é™¤å¤–ã—ãŸæœ‰åŠ¹ãªå€¤ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        valid_values = [v for v in values if pd.notna(v)]
        if len(valid_values) < 2:
            continue
            
        unique_values = len(set(valid_values))
        total_values = len(valid_values)
        
        # --- ã“ã“ã‹ã‚‰ãŒä¿®æ­£ãƒ»ç¢ºèªéƒ¨åˆ†ã§ã™ ---
        if strategy == 'diversity':
            # å¤šæ§˜æ€§é‡è¦–ï¼šå…¨å“¡ç•°ãªã‚‹ãªã‚‰æº€ç‚¹ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ãŒåŠæ•°ä»¥ä¸Šãªã‚‰åŠåˆ†ã®ç‚¹æ•°
            if unique_values == total_values:
                category_score += base_points  # æº€ç‚¹
            elif unique_values / total_values > 0.75:
                category_score += base_points / 2  # åŠåˆ†ã®ç‚¹æ•°ã‚’åŠ ç®—

        elif strategy == 'homogeneity':
            # åŒè³ªæ€§é‡è¦–ï¼šå…¨å“¡åŒã˜å€¤ãªã‚‰æº€ç‚¹ã€å˜ä¸€ã®å€¤ãŒåŠæ•°ä»¥ä¸Šãªã‚‰åŠåˆ†ã®ç‚¹æ•°
            if unique_values == 1:
                category_score += base_points  # æº€ç‚¹
            else:
                # æœ€ã‚‚å¤šã„ã‚«ãƒ†ã‚´ãƒªã®ãƒ¡ãƒ³ãƒãƒ¼æ•°ã‚’å–å¾—
                counts = Counter(valid_values)
                max_count = counts.most_common(1)[0][1]

                # æœ€ã‚‚å¤šã„ã‚«ãƒ†ã‚´ãƒªãŒå…¨ä½“ã®åŠæ•°ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if max_count / total_values > 0.75:
                    category_score += base_points / 2  # åŠåˆ†ã®ç‚¹æ•°ã‚’åŠ ç®—
    
    # 2. ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
    similarity_sum = 0
    for i, idx1 in enumerate(group_indices):
        for idx2 in group_indices[i+1:]:
            similarity_sum += similarity_matrix[idx1][idx2]
    
    # 3. ç·åˆã‚¹ã‚³ã‚¢
    total_score = (w_label * category_score) + (w_sim * similarity_sum)
    return total_score

def find_advanced_optimal_groups(df, similarity_matrix, priority_options, score_map, category_strategies, target_group_size=4, w_sim=1.0):
    """
    ã€ä¿®æ­£ç‰ˆã€‘åå¾©æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
    æŒ‡å®šã•ã‚ŒãŸäººæ•°ã§ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å„ªå…ˆçš„ã«ä½œæˆã—ã€æœ€å¾Œã«ç«¯æ•°å‡¦ç†ã‚’è¡Œã†ã€‚
    """
    st.info(f"å¸Œæœ›ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º {target_group_size}äººã§ã®æœ€é©åŒ–ã‚’é–‹å§‹...")

    available_indices = list(df.index)
    final_groups = []
    group_id_counter = 1
    
    # çµ„ã¿åˆã‚ã›ã®æ¢ç´¢å›æ•°ã®ä¸Šé™ã‚’è¨­å®šï¼ˆè¨ˆç®—æ™‚é–“ã‚’ç¾å®Ÿçš„ã«ã™ã‚‹ãŸã‚ï¼‰
    max_combinations = 2000

    # --- ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ1: ã¾ãš `target_group_size` ã§ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œã‚Šç¶šã‘ã‚‹ ---
    while len(available_indices) >= target_group_size:
        st.write(f"--- {target_group_size}äººã‚°ãƒ«ãƒ¼ãƒ—ã®æ¢ç´¢ (æ®‹ã‚Š {len(available_indices)}äºº) ---")
        best_group = None
        best_score = -float('inf')

        # å…¨å“¡ã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã™ã¨è¨ˆç®—é‡ãŒè†¨å¤§ã«ãªã‚‹ãŸã‚ã€å€™è£œè€…ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æ¢ç´¢ã™ã‚‹
        from itertools import combinations
        import random
        
        sample_size = min(len(available_indices), 40) # æ¢ç´¢å¯¾è±¡ã‚’æœ€å¤§40äººã«çµã‚‹
        if sample_size < target_group_size:
            search_pool = available_indices
        else:
            search_pool = random.sample(available_indices, k=sample_size)

        combinations_count = 0
        for group_indices in combinations(search_pool, target_group_size):
            combinations_count += 1
            if combinations_count > max_combinations:
                st.warning(f"çµ„ã¿åˆã‚ã›ãŒå¤šã™ãã‚‹ãŸã‚ã€{max_combinations}å›ã§æ¢ç´¢ã‚’æ‰“ã¡åˆ‡ã‚Šã¾ã—ãŸã€‚")
                break
            
            score = calculate_advanced_group_score(
                df, list(group_indices), similarity_matrix,
                priority_options, score_map, category_strategies, 1.0, w_sim
            )

            if score > best_score:
                best_score = score
                best_group = list(group_indices)

        # æœ€é©ãªã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ã€å¼·åˆ¶çš„ã«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã™ã‚‹
        if best_group is None:
            st.warning("æœ€é©ãªã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…ˆé ­ã‹ã‚‰å¼·åˆ¶çš„ã«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã—ã¾ã™ã€‚")
            best_group = available_indices[:target_group_size]
            best_score = calculate_advanced_group_score(
                df, best_group, similarity_matrix,
                priority_options, score_map, category_strategies, 1.0, w_sim
            )

        # è¦‹ã¤ã‹ã£ãŸæœ€é©ãªã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¢ºå®šã™ã‚‹
        final_groups.append({
            'members': best_group,
            'score': best_score,
            'size': len(best_group)
        })
        for idx in best_group:
            available_indices.remove(idx)
        st.write(f"âœ… ã‚°ãƒ«ãƒ¼ãƒ— {group_id_counter} ({len(best_group)}äºº) ã‚’ç¢ºå®š (ã‚¹ã‚³ã‚¢: {best_score:.2f})")
        group_id_counter += 1

    # --- ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ2: ç«¯æ•°å‡¦ç† ---
    st.info(f"ç«¯æ•°å‡¦ç†ä¸­... (æ®‹ã‚Š {len(available_indices)}äºº)")
    
    # æœ€å°ã‚°ãƒ«ãƒ¼ãƒ—äººæ•°ã‚’å®šç¾©ï¼ˆã“ã‚Œã‚ˆã‚Šå°‘ãªã„å ´åˆã¯æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«å¸åï¼‰
    # ä¾‹: targetãŒ4ãªã‚‰3äººã€targetãŒ3ãªã‚‰2äºº
    min_final_group_size = max(2, target_group_size - 1)

    if len(available_indices) >= min_final_group_size:
        # æ®‹ã£ãŸãƒ¡ãƒ³ãƒãƒ¼ã§1ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
        rem_group = available_indices.copy()
        score = calculate_advanced_group_score(
            df, rem_group, similarity_matrix,
            priority_options, score_map, category_strategies, 1.0, w_sim
        )
        final_groups.append({
            'members': rem_group,
            'score': score,
            'size': len(rem_group)
        })
        st.write(f"âœ… ã‚°ãƒ«ãƒ¼ãƒ— {group_id_counter} ({len(rem_group)}äºº, ç«¯æ•°ã‚°ãƒ«ãƒ¼ãƒ—) ã‚’ç¢ºå®š (ã‚¹ã‚³ã‚¢: {score:.2f})")
        available_indices.clear()

    # --- ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ3: ãã‚Œã§ã‚‚æ®‹ã£ãŸå°‘æ•°ã®ãƒ¡ãƒ³ãƒãƒ¼ã‚’æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«å‰²ã‚Šå½“ã¦ã‚‹ ---
    if available_indices:
        st.info(f"æœ€çµ‚çš„ãªæ®‹ã‚Šãƒ¡ãƒ³ãƒãƒ¼ {len(available_indices)}äººã‚’æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«å‰²ã‚Šå½“ã¦ã¾ã™ã€‚")
        assign_remaining_members_advanced(df, available_indices, final_groups, similarity_matrix, priority_options, score_map, category_strategies, w_sim)

    st.success(f"æœ€é©åŒ–å®Œäº†: {len(final_groups)} ã‚°ãƒ«ãƒ¼ãƒ—ãŒä½œæˆã•ã‚Œã¾ã—ãŸ")
    return final_groups

def assign_remaining_members_advanced(df, remaining_indices, groups, similarity_matrix, priority_options, score_map, category_strategies, w_sim):
    """
    ã€ä¿®æ­£ç‰ˆã€‘æ®‹å­˜ãƒ¡ãƒ³ãƒãƒ¼ã‚’æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«å‰²ã‚Šå½“ã¦
    â˜… å¼•æ•°ã®ä¸ä¸€è‡´ã‚’ä¿®æ­£
    """
    st.info(f"æ®‹ã‚Š {len(remaining_indices)} äººã‚’æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«å‰²ã‚Šå½“ã¦ä¸­...")
    
    for member_idx in remaining_indices:
        best_group_idx = -1
        best_score_increase = -float('inf')
        
        for group_idx, group in enumerate(groups):
            # ãƒ¡ãƒ³ãƒãƒ¼ã‚’è¿½åŠ ã—ãŸå ´åˆã®ã‚¹ã‚³ã‚¢å¢—åˆ†ã‚’è¨ˆç®—
            original_score = group.get('score', 0) # scoreã‚­ãƒ¼ãŒãªã„å ´åˆã‚‚è€ƒæ…®
            extended_group = group['members'] + [member_idx]
            
            # calculate_advanced_group_scoreã®å¼•æ•°ã‚’ä¿®æ­£æ¸ˆã¿ã®ã‚‚ã®ã«åˆã‚ã›ã‚‹
            new_score = calculate_advanced_group_score(
                df, extended_group, similarity_matrix, 
                priority_options, score_map, category_strategies, 1.0, w_sim
            )
            score_increase = new_score - original_score
            
            if score_increase > best_score_increase:
                best_score_increase = score_increase
                best_group_idx = group_idx
        
        if best_group_idx >= 0:
            groups[best_group_idx]['members'].append(member_idx)
            # ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚¹ã‚³ã‚¢ã¨ã‚µã‚¤ã‚ºã‚‚æ›´æ–°ã™ã‚‹
            groups[best_group_idx]['score'] = calculate_advanced_group_score(
                df, groups[best_group_idx]['members'], similarity_matrix,
                priority_options, score_map, category_strategies, 1.0, w_sim
            )
            groups[best_group_idx]['size'] += 1
            st.write(f"ãƒ¡ãƒ³ãƒãƒ¼è¿½åŠ : ã‚°ãƒ«ãƒ¼ãƒ— {best_group_idx + 1} ã«å‰²ã‚Šå½“ã¦ (ã‚¹ã‚³ã‚¢å¢—åˆ†: {best_score_increase:.2f})")

def create_advanced_results_dataframe(df, groups):
    """é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°çµæœã‚’DataFrameã¨ã—ã¦ç”Ÿæˆ"""
    if not groups:
        return None
    
    result_df = df.copy()
    result_df['advanced_group_id'] = 0
    result_df['advanced_group_score'] = 0.0
    
    for group_idx, group in enumerate(groups):
        for member_idx in group['members']:
            result_df.loc[member_idx, 'advanced_group_id'] = group_idx + 1
            result_df.loc[member_idx, 'advanced_group_score'] = group['score']
    
    return result_df

def display_advanced_group_analysis(df, groups, priority_options, score_map, category_strategies):
    """é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°çµæœã®åˆ†æè¡¨ç¤º"""
    if not groups:
        st.warning("ã‚°ãƒ«ãƒ¼ãƒ—ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    st.subheader("ğŸ“Š é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°åˆ†æçµæœ")
    
    # æˆ¦ç•¥æƒ…å ±ã®è¡¨ç¤º
    st.write("**é©ç”¨ã•ã‚ŒãŸæˆ¦ç•¥:**")
    strategy_info = []
    for cat_num, strategy in category_strategies.items():
        strategy_name = "å¤šæ§˜æ€§é‡è¦–" if strategy == "diversity" else "åŒè³ªæ€§é‡è¦–"
        strategy_info.append(f"ã‚«ãƒ†ã‚´ãƒª{cat_num}: {strategy_name}")
    st.write(" | ".join(strategy_info))
    
    # åŸºæœ¬çµ±è¨ˆ
    total_members = sum(group['size'] for group in groups)
    avg_group_size = total_members / len(groups)
    total_score = sum(group['score'] for group in groups)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç·ã‚°ãƒ«ãƒ¼ãƒ—æ•°", len(groups))
    with col2:
        st.metric("ç·ãƒ¡ãƒ³ãƒãƒ¼æ•°", total_members)
    with col3:
        st.metric("å¹³å‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º", f"{avg_group_size:.1f}")
    with col4:
        st.metric("ç·åˆã‚¹ã‚³ã‚¢", f"{total_score:.1f}")
    
    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°
    for idx, group in enumerate(groups):
        with st.expander(f"ã‚°ãƒ«ãƒ¼ãƒ— {idx + 1} (ã‚¹ã‚³ã‚¢: {group['score']:.2f}, {group['size']}äºº)"):
            group_df = df.iloc[group['members']]
            st.dataframe(group_df)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
            st.write("**ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ:**")
            for priority in priority_options:
                if priority in df.columns:
                    col_values = group_df[priority].value_counts()
                    cat_num = int(priority.replace('cat', '').replace('_group', ''))
                    strategy = category_strategies.get(cat_num, 'diversity')
                    strategy_name = "å¤šæ§˜æ€§é‡è¦–" if strategy == "diversity" else "åŒè³ªæ€§é‡è¦–"
                    
                    st.write(f"- {priority} ({strategy_name}): {dict(col_values)}")

def main():
    st.title("ã‚«ãƒ†ã‚´ãƒªå„ªå…ˆåº¦ä»˜ãã‚·ãƒ£ãƒƒãƒ•ãƒ« + ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦å¯¾å¿œ")
    
    # Azure OpenAIè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
    ENDPOINT = os.getenv("ENDPOINT")
    API_KEY = os.getenv("API_KEY")
    
    # ENDPOINTã‹ã‚‰base URLã‚’æŠ½å‡º
    if ENDPOINT:
        # https://openai-shuffle.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2023-05-15
        # ã‹ã‚‰ https://openai-shuffle.openai.azure.com/ ã‚’æŠ½å‡º
        import re
        match = re.match(r'(https://[^/]+)', ENDPOINT)
        AZURE_ENDPOINT = match.group(1) + "/" if match else None
        
        # API versionã¨deployment nameã‚’æŠ½å‡º
        if 'api-version=' in ENDPOINT:
            API_VERSION = ENDPOINT.split('api-version=')[1]
        else:
            API_VERSION = "2023-05-15"
            
        if '/deployments/' in ENDPOINT:
            DEPLOYMENT_NAME = ENDPOINT.split('/deployments/')[1].split('/')[0]
        else:
            DEPLOYMENT_NAME = "text-embedding-3-large"
    else:
        AZURE_ENDPOINT = None
        API_VERSION = "2023-05-15"
        DEPLOYMENT_NAME = "text-embedding-3-large"
    
    # ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
    if not ENDPOINT or not API_KEY:
        st.error("âš ï¸ ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env.localãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.write("å¿…è¦ãªç’°å¢ƒå¤‰æ•°:")
        st.write("- ENDPOINT")
        st.write("- API_KEY")
        client = None
    else:
        # Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        try:
            client = AzureOpenAI(
                azure_endpoint=AZURE_ENDPOINT,
                api_key=API_KEY,
                api_version=API_VERSION
            )
            st.success(f"âœ… Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
            st.info(f"ä½¿ç”¨ä¸­ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ: {DEPLOYMENT_NAME}")
        except Exception as e:
            st.error(f"âŒ Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            client = None

    # 1) ã‚¨ã‚¯ã‚»ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader("ã‚¨ã‚¯ã‚»ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xlsx", "xls"])
    if uploaded_file is None:
        st.stop()

    # DataFrame ã¸èª­ã¿è¾¼ã¿
    try:
        df = pd.read_excel(uploaded_file)
    except Exception as e:
        st.error(f"Excelã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()

    st.write("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿:")
    st.dataframe(df)
    
    # ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®è‡ªå‹•æ¤œå‡ºã¨æ‰‹å‹•èª¿æ•´
    st.subheader("åˆ—ã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ†æãƒ»é¸æŠ")
    
    # è‡ªå‹•æ¤œå‡ºå®Ÿè¡Œ
    detected_text_columns, analysis_results = auto_detect_text_columns(df)
    
    # åˆ†æçµæœã®è¡¨ç¤º
    st.write("**å„åˆ—ã®è‡ªå‹•åˆ¤å®šçµæœ:**")
    for col_name, result in analysis_results.items():
        icon = "ğŸ“" if result["type"] == "free_text" else "ğŸ·ï¸"
        color = "green" if result["type"] == "free_text" else "blue"
        st.markdown(f":{color}[{icon} {col_name}: **{result['type']}**] - {result['reason']}")
    
    # ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆè‡ªå‹•æ¤œå‡ºçµæœã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
    text_column_options = [f"åˆ—{i+1}: {col}" for i, col in enumerate(df.columns)]
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠã®æ±ºå®š
    default_selection = "ãªã—"
    if len(detected_text_columns) > 0:
        # æœ€åˆã«æ¤œå‡ºã•ã‚ŒãŸãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
        default_index = detected_text_columns[0]
        default_selection = f"åˆ—{default_index+1}: {df.columns[default_index]}"
    
    st.write("**ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠ:**")
    selected_text_columns = st.multiselect(
        "è‡ªå‹•åˆ¤å®šçµæœã‚’ç¢ºèªã—ã¦ã€ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨ã™ã‚‹åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°é¸æŠå¯èƒ½ï¼‰",
        text_column_options,
        default=[default_selection] if default_selection != "ãªã—" else []
    )
    
    use_text_similarity = len(selected_text_columns) > 0
    text_column_indices = []
    
    if use_text_similarity:
        for selected_col in selected_text_columns:
            col_index = int(selected_col.split(":")[0].replace("åˆ—", "")) - 1
            text_column_indices.append(col_index)
            
            # è‡ªå‹•åˆ¤å®šã¨ç•°ãªã‚‹é¸æŠã‚’ã—ãŸå ´åˆã®ç¢ºèª
            if col_index not in detected_text_columns:
                st.warning(f"âš ï¸ æ³¨æ„: {df.columns[col_index]} ã¯è‡ªå‹•åˆ¤å®šã§ã¯ã€Œã‚¿ã‚°ç³»ã€ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸãŒã€ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚")
            else:
                st.success(f"âœ… è‡ªå‹•åˆ¤å®šã¨ä¸€è‡´: {df.columns[col_index]}")
        
        st.write(f"**ä½¿ç”¨ã™ã‚‹ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ—:** {[df.columns[i] for i in text_column_indices]}")
    else:
        st.info("â„¹ï¸ ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã¯ä½¿ç”¨ã›ãšã€ã‚¿ã‚°ç³»ã‚«ãƒ†ã‚´ãƒªã®ã¿ã§å‡¦ç†ã—ã¾ã™ã€‚")

    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹
    cat1_data = df.iloc[:, 1]  # 2åˆ—ç›®
    cat2_data = df.iloc[:, 2]  # 3åˆ—ç›®
    cat3_data = df.iloc[:, 3]  # 4åˆ—ç›®
    
    # ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆè¤‡æ•°åˆ—å¯¾å¿œï¼‰
    text_data_list = []
    if use_text_similarity and text_column_indices:
        for col_idx in text_column_indices:
            text_data_list.append(df.iloc[:, col_idx])

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã‚’å–å¾—
    cat1_unique = cat1_data.dropna().unique().tolist()
    cat2_unique = cat2_data.dropna().unique().tolist()
    cat3_unique = cat3_data.dropna().unique().tolist()

    st.subheader("ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ã‚°ãƒ«ãƒ¼ãƒ—è¨­å®š")

    cat1_groups = group_selection(cat1_unique, label="ã‚«ãƒ†ã‚´ãƒª1")
    cat2_groups = group_selection(cat2_unique, label="ã‚«ãƒ†ã‚´ãƒª2")
    cat3_groups = group_selection(cat3_unique, label="ã‚«ãƒ†ã‚´ãƒª3")
    
    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®æˆ¦ç•¥è¨­å®š
    st.subheader("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ç‚¹æ•°é…åˆ†æˆ¦ç•¥")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        cat1_strategy = st.selectbox(
            "ã‚«ãƒ†ã‚´ãƒª1ã®æˆ¦ç•¥",
            ["diversity", "homogeneity"],
            format_func=lambda x: "å¤šæ§˜æ€§é‡è¦–ï¼ˆç•°ãªã‚‹å€¤ã§é«˜å¾—ç‚¹ï¼‰" if x == "diversity" else "åŒè³ªæ€§é‡è¦–ï¼ˆåŒã˜å€¤ã§é«˜å¾—ç‚¹ï¼‰"
        )
    with col2:
        cat2_strategy = st.selectbox(
            "ã‚«ãƒ†ã‚´ãƒª2ã®æˆ¦ç•¥", 
            ["diversity", "homogeneity"],
            format_func=lambda x: "å¤šæ§˜æ€§é‡è¦–ï¼ˆç•°ãªã‚‹å€¤ã§é«˜å¾—ç‚¹ï¼‰" if x == "diversity" else "åŒè³ªæ€§é‡è¦–ï¼ˆåŒã˜å€¤ã§é«˜å¾—ç‚¹ï¼‰"
        )
    with col3:
        cat3_strategy = st.selectbox(
            "ã‚«ãƒ†ã‚´ãƒª3ã®æˆ¦ç•¥",
            ["diversity", "homogeneity"], 
            format_func=lambda x: "å¤šæ§˜æ€§é‡è¦–ï¼ˆç•°ãªã‚‹å€¤ã§é«˜å¾—ç‚¹ï¼‰" if x == "diversity" else "åŒè³ªæ€§é‡è¦–ï¼ˆåŒã˜å€¤ã§é«˜å¾—ç‚¹ï¼‰"
        )

    # ã‚«ãƒ©ãƒ è¿½åŠ : cat1_group, cat2_group, cat3_group
    df["cat1_group"] = assign_groups(cat1_data, cat1_groups)
    df["cat2_group"] = assign_groups(cat2_data, cat2_groups)
    df["cat3_group"] = assign_groups(cat3_data, cat3_groups)
    
    # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚°ãƒ«ãƒ¼ãƒ—æ©Ÿèƒ½ã¯å‰Šé™¤ï¼ˆé«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã§ç›´æ¥å‡¦ç†ï¼‰

    st.write("ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘çµæœ (catX_groupåˆ—ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸ):")
    st.dataframe(df)

    # å„ªå…ˆé †ä½è¨­å®š
    st.subheader("å„ªå…ˆé †ä½è¨­å®š")
    
    # åŸºæœ¬ã®3ã‚«ãƒ†ã‚´ãƒªã®ã¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã¯é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã§ç›´æ¥å‡¦ç†ï¼‰
    priority_options = ["cat1_group", "cat2_group", "cat3_group"]
        
    st.write("åˆ©ç”¨å¯èƒ½ãªè¦ç´ :")
    st.write("- cat1_group: ã‚«ãƒ†ã‚´ãƒª1ã®ã‚°ãƒ«ãƒ¼ãƒ—")
    st.write("- cat2_group: ã‚«ãƒ†ã‚´ãƒª2ã®ã‚°ãƒ«ãƒ¼ãƒ—") 
    st.write("- cat3_group: ã‚«ãƒ†ã‚´ãƒª3ã®ã‚°ãƒ«ãƒ¼ãƒ—")
    
    priorities = st.multiselect(
        "å„ªå…ˆé †ä½ã‚’è¨­å®šã—ã¦ãã ã•ã„ (ä¸Šã‹ã‚‰é †ã« 7ç‚¹, 5ç‚¹, 3ç‚¹, 1ç‚¹)",
        priority_options,
        default=priority_options
    )
    
    # ç‚¹æ•°ãƒãƒƒãƒ— (7, 5, 3, 1ç‚¹ã‚·ã‚¹ãƒ†ãƒ )
    score_map = {}
    score_values = [7, 5, 3, 1]
    for i, priority in enumerate(priorities):
        if i < len(score_values):
            score_map[priority] = score_values[i]

    st.write("å„ªå…ˆé †ä½:", priorities)
    st.write("ç‚¹æ•°ãƒãƒƒãƒ—:", score_map)

    # é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°è¨­å®š
    st.subheader("ğŸš€ é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°è¨­å®š")
    
    # é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã®ã¿ä½¿ç”¨
    use_advanced_grouping = True
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆé«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ç”¨ï¼‰
    st.write("**é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ç”¨ãƒ†ã‚­ã‚¹ãƒˆåˆ—è¨­å®š:**")
        
    # é¡ä¼¼åº¦è¨ˆç®—ãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
    similarity_mode = st.radio(
        "é¡ä¼¼åº¦è¨ˆç®—ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
        ["å˜ä¸€åˆ—ã§ã®é¡ä¼¼åº¦", "2åˆ—çµåˆã§ã®é¡ä¼¼åº¦"],
        help="å˜ä¸€åˆ—ï¼š1ã¤ã®åˆ—å†…ã§ã®ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ / 2åˆ—çµåˆï¼š2ã¤ã®åˆ—ã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦"
    )
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—1ã®é¸æŠ
    text_col1_options = [f"åˆ—{i+1}: {col}" for i, col in enumerate(df.columns)]
    selected_text_col1 = st.selectbox(
        "ãƒ†ã‚­ã‚¹ãƒˆåˆ—1ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã‚¹ã‚­ãƒ«æƒ…å ±ï¼‰",
        text_col1_options,
        index=1 if len(df.columns) > 1 else 0
    )
    text_col1_index = int(selected_text_col1.split(":")[0].replace("åˆ—", "")) - 1
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—2ã®é¸æŠï¼ˆ2åˆ—çµåˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿ï¼‰
    text_col2_index = None
    if similarity_mode == "2åˆ—çµåˆã§ã®é¡ä¼¼åº¦":
        selected_text_col2 = st.selectbox(
            "ãƒ†ã‚­ã‚¹ãƒˆåˆ—2ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šè‡ªå·±PRï¼‰",
            text_col1_options,
            index=2 if len(df.columns) > 2 else 0
        )
        text_col2_index = int(selected_text_col2.split(":")[0].replace("åˆ—", "")) - 1
    
    # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã®é‡ã¿è¨­å®š
    st.write("**é‡ã¿è¨­å®š:**")
    w_sim = st.slider("ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã®é‡ã¿", 0, 10, 1, 1)
    
    # å›ºå®šã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º
    target_group_size = st.number_input("å¸Œæœ›ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º", min_value=2, value=4, step=1)
    
    st.write(f"**è¨­å®šç¢ºèª:**")
    st.write(f"- é¡ä¼¼åº¦è¨ˆç®—ãƒ¢ãƒ¼ãƒ‰: {similarity_mode}")
    st.write(f"- ãƒ†ã‚­ã‚¹ãƒˆåˆ—1: {df.columns[text_col1_index]}")
    if text_col2_index is not None:
        st.write(f"- ãƒ†ã‚­ã‚¹ãƒˆåˆ—2: {df.columns[text_col2_index]}")
    st.write(f"- ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã®é‡ã¿: {w_sim}")


    # dfã«å›ºå®šãƒ•ãƒ©ã‚°, best_group_id, chunk_id ãŒãªã‘ã‚Œã°ä½œæˆ
    if "fixed" not in df.columns:
        df["fixed"] = False  # æœ€é«˜ç‚¹æº€ç‚¹ã§å›ºå®šã•ã‚ŒãŸã‹ã©ã†ã‹
    if "best_group_id" not in df.columns:
        df["best_group_id"] = np.nan
    if "chunk_id" not in df.columns:
        df["chunk_id"] = np.nan
    

    # å®Ÿè¡Œãƒœã‚¿ãƒ³ï¼ˆé«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã®ã¿ï¼‰
    execute_button = st.button("ğŸš€ é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°å®Ÿè¡Œ")
    
    if execute_button:
        # é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            st.info("ğŸš€ é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
            
            # é¡ä¼¼åº¦è¡Œåˆ—ã®ä½œæˆ
            similarity_matrix = create_advanced_similarity_matrix(
                df, text_col1_index, text_col2_index, client, DEPLOYMENT_NAME
            )
            
            if similarity_matrix is not None:
                # é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
                # ã‚«ãƒ†ã‚´ãƒªæˆ¦ç•¥è¾æ›¸ã‚’ä½œæˆ
                category_strategies = {
                    1: cat1_strategy,  # cat1_groupåˆ—ç”¨ã®æˆ¦ç•¥
                    2: cat2_strategy,  # cat2_groupåˆ—ç”¨ã®æˆ¦ç•¥ 
                    3: cat3_strategy   # cat3_groupåˆ—ç”¨ã®æˆ¦ç•¥
                }
                
                advanced_groups = find_advanced_optimal_groups(
                    df, similarity_matrix, priority_options, score_map,
                    category_strategies, target_group_size, w_sim # min_group_size -> target_group_size ã«ä¿®æ­£
                )
                
                if advanced_groups:
                    # çµæœã®è¡¨ç¤º
                    display_advanced_group_analysis(df, advanced_groups, priority_options, score_map, category_strategies)
                    
                    # çµæœDataFrameã®ä½œæˆ
                    advanced_result_df = create_advanced_results_dataframe(df, advanced_groups)
                    if advanced_result_df is not None:
                        st.subheader("ğŸ“Š é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°çµæœ")
                        st.dataframe(advanced_result_df)
                        
                        # CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        csv_data = advanced_result_df.to_csv(index=False, encoding="shift_jis")
                        st.download_button(
                            "ğŸ“¥ é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°çµæœCSV (Shift-JIS)",
                            data=csv_data.encode("shift_jis"),
                            file_name="advanced_grouping_results.csv",
                            mime="text/csv"
                        )
                else:
                    st.error("é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            else:
                st.error("é¡ä¼¼åº¦è¡Œåˆ—ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¾“æ¥æ–¹å¼ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
        
if __name__ == "__main__":
    main()